{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e685e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, expr, size, collect_list, explode, array_intersect, lit\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d80924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_als_model_with_tuning(ratings):\n",
    "    # Define ALS model\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True\n",
    "    )\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(als.rank, [10, 20, 30]) \\\n",
    "        .addGrid(als.regParam, [0.01, 0.1, 0.2]) \\\n",
    "        .build()\n",
    "\n",
    "    # Define evaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "\n",
    "    # Define cross-validator\n",
    "    cross_validator = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5,\n",
    "        parallelism=2\n",
    "    )\n",
    "\n",
    "    # Train the model with cross-validation\n",
    "    cv_model = cross_validator.fit(ratings)\n",
    "\n",
    "    return cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97fdce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(model, n_recommendations=100):\n",
    "    user_recs = model.recommendForAllUsers(n_recommendations)\n",
    "    return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc7cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map(top_movies, ratings, n_recommendations=100):\n",
    "    movie_id_expr = get_movie_id(top_movies, n_recommendations)\n",
    "    user_actual_movies = ratings.groupBy(\"userId\").agg(\n",
    "        expr(\"collect_list(movieId) as actual_movies\")\n",
    "    )\n",
    "    \n",
    "    precision_per_user = user_actual_movies.select(\n",
    "        expr(f\"\"\"size(array_intersect(actual_movies, {movie_id_expr})) as hits\"\"\"),\n",
    "        size(col(\"actual_movies\")).alias(\"total_relevant\"),\n",
    "        lit(n_recommendations).alias(\"total_recommendations\")\n",
    "    ).selectExpr(\n",
    "        \"hits / total_recommendations as precision_at_k\"\n",
    "    )\n",
    "    \n",
    "    mean_average_precision = precision_per_user.selectExpr(\n",
    "        \"avg(precision_at_k) as MAP\"\n",
    "    ).first()[\"MAP\"]\n",
    "    \n",
    "    return mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9241ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id(top_movies, n_recommendations=100):\n",
    "    top_movie_ids = top_movies.select(explode(\"recommendations.movieId\").alias(\"movieId\")).distinct().limit(n_recommendations).collect()\n",
    "    return f\"array({','.join([str(row['movieId']) for row in top_movie_ids])})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5065f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(spark):\n",
    "    base_path = f'./ml-latest'\n",
    "    train_path = f'{base_path}/train_ratings.parquet'\n",
    "    val_path = f'{base_path}/val_ratings.parquet'\n",
    "    test_path = f'{base_path}/test_ratings.parquet'\n",
    "\n",
    "    train_ratings = spark.read.parquet(train_path, header=True, inferSchema=True)\n",
    "    val_ratings = spark.read.parquet(val_path, header=True, inferSchema=True)\n",
    "    test_ratings = spark.read.parquet(test_path, header=True, inferSchema=True)\n",
    "\n",
    "    als_model = train_als_model_with_tuning(train_ratings)\n",
    "    top_recommendations = get_top_n_recommendations(als_model)\n",
    "\n",
    "    train_map = compute_map(top_recommendations, train_ratings)\n",
    "    print(f\"Train MAP: {train_map}\")\n",
    "    val_map = compute_map(top_recommendations, val_ratings)\n",
    "    print(f\"Validation MAP: {val_map}\")\n",
    "    test_map = compute_map(top_recommendations, test_ratings)\n",
    "    print(f\"Test MAP: {test_map}\")\n",
    "\n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e726f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(spark):\n",
    "    process_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aad8c51a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAP: 0.003450595031377944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAP: 0.0007368802062188479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAP: 0.0007785544136996614\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('als_recommender') \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"800\") \\\n",
    "        .config(\"spark.executor.memory\", \"16g\") \\\n",
    "        .config(\"spark.driver.memory\", \"16g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "        .getOrCreate()\n",
    "#     userID = os.getenv('USER')\n",
    "    main(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
