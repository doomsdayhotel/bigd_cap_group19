{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e685e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, expr, size, collect_list, explode, array_intersect, lit\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badece88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPEvaluator(Evaluator):\n",
    "    def __init__(self, predictionCol=\"prediction\", labelCol=\"movieId\", userCol=\"userId\", k=10):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.userCol = userCol\n",
    "        self.k = k\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        # Assumes predictions are already made and contain columns for user, item, and prediction\n",
    "        windowSpec = Window.partitionBy(self.userCol).orderBy(col(self.predictionCol).desc())\n",
    "        perUserPredictedItemsDF = dataset \\\n",
    "            .select(self.userCol, self.labelCol, self.predictionCol, F.rank().over(windowSpec).alias('rank')) \\\n",
    "            .where('rank <= {}'.format(self.k)) \\\n",
    "            .groupBy(self.userCol) \\\n",
    "            .agg(collect_list(self.labelCol).alias('items'))\n",
    "\n",
    "        actualItemsDF = dataset \\\n",
    "            .groupBy(self.userCol) \\\n",
    "            .agg(collect_list(self.labelCol).alias('actualItems'))\n",
    "\n",
    "        resultDF = perUserPredictedItemsDF.join(actualItemsDF, self.userCol) \\\n",
    "            .select(expr(\"size(array_intersect(items, actualItems)) as hits\"), \"items\", \"actualItems\")\n",
    "\n",
    "        MAP = resultDF.select((col(\"hits\") / self.k).alias(\"precision_at_k\")).agg(F.avg(\"precision_at_k\")).first()[0]\n",
    "        \n",
    "        return MAP\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d80924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_als_model_with_tuning(ratings):\n",
    "    # Define ALS model\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True\n",
    "    )\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(als.rank, [10, 20, 30]) \\\n",
    "        .addGrid(als.regParam, [0.01, 0.1, 0.2]) \\\n",
    "        .build()\n",
    "\n",
    "    # Define evaluator\n",
    "    map_evaluator = MAPEvaluator(predictionCol=\"prediction\", labelCol=\"movieId\", userCol=\"userId\", k=10)\n",
    "\n",
    "    # Define cross-validator\n",
    "    cross_validator = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=map_evaluator,\n",
    "        numFolds=5,\n",
    "        parallelism=2\n",
    "    )\n",
    "\n",
    "    # Train the model with cross-validation\n",
    "    cv_model = cross_validator.fit(ratings)\n",
    "\n",
    "    return cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97fdce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(model, n_recommendations=100):\n",
    "    user_recs = model.recommendForAllUsers(n_recommendations)\n",
    "    return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc7cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map(top_movies, ratings, n_recommendations=100):\n",
    "    movie_id_expr = get_movie_id(top_movies, n_recommendations)\n",
    "    user_actual_movies = ratings.groupBy(\"userId\").agg(\n",
    "        expr(\"collect_list(movieId) as actual_movies\")\n",
    "    )\n",
    "    \n",
    "    precision_per_user = user_actual_movies.select(\n",
    "        expr(f\"\"\"size(array_intersect(actual_movies, {movie_id_expr})) as hits\"\"\"),\n",
    "        size(col(\"actual_movies\")).alias(\"total_relevant\"),\n",
    "        lit(n_recommendations).alias(\"total_recommendations\")\n",
    "    ).selectExpr(\n",
    "        \"hits / total_recommendations as precision_at_k\"\n",
    "    )\n",
    "    \n",
    "    mean_average_precision = precision_per_user.selectExpr(\n",
    "        \"avg(precision_at_k) as MAP\"\n",
    "    ).first()[\"MAP\"]\n",
    "    \n",
    "    return mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9241ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id(top_movies, n_recommendations=100):\n",
    "    top_movie_ids = top_movies.select(explode(\"recommendations.movieId\").alias(\"movieId\")).distinct().limit(n_recommendations).collect()\n",
    "    return f\"array({','.join([str(row['movieId']) for row in top_movie_ids])})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5065f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(spark):\n",
    "    base_path = f'./ml-latest'\n",
    "    train_path = f'{base_path}/train_ratings.parquet'\n",
    "    val_path = f'{base_path}/val_ratings.parquet'\n",
    "    test_path = f'{base_path}/test_ratings.parquet'\n",
    "\n",
    "    train_ratings = spark.read.parquet(train_path, header=True, inferSchema=True)\n",
    "    val_ratings = spark.read.parquet(val_path, header=True, inferSchema=True)\n",
    "    test_ratings = spark.read.parquet(test_path, header=True, inferSchema=True)\n",
    "\n",
    "    als_model = train_als_model_with_tuning(train_ratings)\n",
    "    top_recommendations = get_top_n_recommendations(als_model)\n",
    "\n",
    "    train_map = compute_map(top_recommendations, train_ratings)\n",
    "    print(f\"Train MAP: {train_map}\")\n",
    "    val_map = compute_map(top_recommendations, val_ratings)\n",
    "    print(f\"Validation MAP: {val_map}\")\n",
    "    test_map = compute_map(top_recommendations, test_ratings)\n",
    "    print(f\"Test MAP: {test_map}\")\n",
    "\n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e726f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(spark):\n",
    "    process_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad8c51a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/05/15 14:56:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 14:56:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/15 14:56:21 WARN BlockManager: Block rdd_10_0 already exists on this machine; not re-adding it\n",
      "24/05/15 14:56:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/15 14:56:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                7]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAP: 1.2864221323302808e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAP: 2.9348033437192777e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAP: 3.2771970671531937e-06\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('als_recommender') \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"800\") \\\n",
    "        .config(\"spark.executor.memory\", \"16g\") \\\n",
    "        .config(\"spark.driver.memory\", \"16g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "        .getOrCreate()\n",
    "#     userID = os.getenv('USER')\n",
    "    main(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
