{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182ed4eb",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e858ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import collect_list, udf, col, max, size\n",
    "from pyspark.ml.feature import CountVectorizer, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(spark):\n",
    "\n",
    "    '''1. Preprocessing Data '''\n",
    "    # Load the ratings.csv into DataFrame\n",
    "    ratings_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/ml-latest/ratings.csv', schema='userId INT, movieId STRING, rating FLOAT, timestamp BIGINT')\n",
    "    ratings_df = ratings_df.repartition(256, \"timestamp\")\n",
    "#     ratings_df.cache() #Cache for optimizing\n",
    "    \n",
    "    # Group by userId and collect all movieIds into a list\n",
    "    ratings_df_grouped = ratings_df.groupBy(\"userId\").agg(collect_list(\"movieId\").alias(\"movieIds\")).cache()\n",
    "    # ratings_df_grouped = ratings_df_grouped.repartition(\"userId\")\n",
    "#     ratings_df_grouped.cache() #Cache for optimizing\n",
    "#     ratings_df_grouped.show()\n",
    "    \n",
    "    ratings_df_filtered = ratings_df_grouped.filter(size(\"movieIds\") >= 5)\n",
    "    \n",
    "    # Vectorize moviIds\n",
    "    cv = CountVectorizer(inputCol = 'movieIds', outputCol = 'features')\n",
    "    model = cv.fit(ratings_df_filtered)\n",
    "    ratings_df_final = model.transform(ratings_df_filtered)\n",
    "    # ratings_df_final.show()\n",
    "    \n",
    "\n",
    "    ''' 2. Applying MinHash '''\n",
    "    mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "    model = mh.fit(ratings_df_final)\n",
    "\n",
    "    print(\"Transformed Data\\n\")\n",
    "    transformed_df = model.transform(ratings_df_final)\n",
    "#     transformed_df.show()\n",
    "    similar_pairs = model.approxSimilarityJoin(transformed_df, transformed_df, 0.6, distCol=\"JaccardDistance\")\n",
    "    # similar_pairs = similar_pairs.filter(\"datasetA.userId < datasetB.userId\")\n",
    "#     similar_pairs.show()\n",
    "    \n",
    "\n",
    "    print(\"100 similarity pairs\\n\")\n",
    "#     similar_pairs = similar_pairs.filter(\"JaccardDistance == 0\").filter(\"datasetA.userId < datasetB.userId\")\n",
    "    similar_pairs = similar_pairs.filter(\"datasetA.userId < datasetB.userId\").orderBy(\"JaccardDistance\", ascending=True).limit(100)\n",
    "    # top_100_pairs.select(\"datasetA.userId\", \"datasetB.userId\", \"JaccardDistance\").show(100)\n",
    "    # top_100_pairs.printSchema()\n",
    "    \n",
    "    \n",
    "    print(\"Simplified df\\n\")\n",
    "    simplified_df = similar_pairs.select(\n",
    "        col(\"datasetA.userId\").alias(\"userIdA\"),\n",
    "        col(\"datasetB.userId\").alias(\"userIdB\"),\n",
    "        \"JaccardDistance\"\n",
    "    )\n",
    "\n",
    "    print(\"Write simplified df to Parquet\\n\")\n",
    "    # Write the simplified DataFrame to parquet\n",
    "#     simplified_df.show()\n",
    "    simplified_df.write.parquet('top100pairs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only enter this block if we're in main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create the spark session object\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"minHash\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#     spark = SparkSession.builder \\\n",
    "#                         .appName('minHash') \\\n",
    "#                         .getOrCreate()\n",
    "\n",
    "    # Call our main routine\n",
    "    main(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff576f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''convert parquet to csv and store result'''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read and Process Parquet Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read Parquet files from a specified path or multiple paths\n",
    "df = spark.read.parquet(\"top100pairs_all\")\n",
    "\n",
    "\n",
    "# Order by a specific column and limit to 100 records\n",
    "result_df = df.orderBy(\"userIdA\", \"userIdB\", ascending=True).limit(100)\n",
    "\n",
    "# Show the results\n",
    "result_df.write.csv('q1_results', header=True)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac3d6c",
   "metadata": {},
   "source": [
    "### Q1 test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697069b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+--------------------+--------------------+------------------+\n",
    "|            datasetA|            datasetB|   JaccardDistance|\n",
    "+--------------------+--------------------+------------------+\n",
    "|{159361, [50, 318...|{6820, [318, 4995...|0.5555555555555556|\n",
    "|{112094, [1196, 1...|{112094, [1196, 1...|               0.0|\n",
    "|{254602, [48, 150...|{254602, [48, 150...|               0.0|\n",
    "|{257335, [260, 31...|{257335, [260, 31...|               0.0|\n",
    "|{288760, [1, 47, ...|{288760, [1, 47, ...|               0.0|\n",
    "|{214820, [2028, 2...|{214820, [2028, 2...|               0.0|\n",
    "|{77749, [223, 296...|{77749, [223, 296...|               0.0|\n",
    "|{26650, [318, 527...|{85346, [260, 527...|0.5151515151515151|\n",
    "|{131756, [3578, 3...|{214189, [260, 52...|0.5227272727272727|\n",
    "|{185669, [318, 35...|{185669, [318, 35...|               0.0|\n",
    "|{278829, [10, 18,...|{278829, [10, 18,...|               0.0|\n",
    "|{220550, [260, 29...|{220550, [260, 29...|               0.0|\n",
    "|{137155, [19, 165...|{137155, [19, 165...|               0.0|\n",
    "|{93424, [260, 356...|{93424, [260, 356...|               0.0|\n",
    "|{131505, [260, 29...|{131505, [260, 29...|               0.0|\n",
    "|{271048, [6, 50, ...|{271048, [6, 50, ...|               0.0|\n",
    "|{200191, [47, 185...|{200191, [47, 185...|               0.0|\n",
    "|{10131, [44, 70, ...|{10131, [44, 70, ...|               0.0|\n",
    "|{276465, [277, 34...|{276465, [277, 34...|               0.0|\n",
    "|{98347, [2302], (...|{319979, [1639, 2...|               0.5|\n",
    "+--------------------+--------------------+------------------+\n",
    "only showing top 20 rows\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20237493",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901891fa",
   "metadata": {},
   "source": [
    "### Q2 test output (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4cb3aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6ffbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''small top 100 correlation'''\n",
    "\n",
    "def main(spark):\n",
    "\n",
    "    # 1. Read & Transform the data\n",
    "    similarity_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/small_top100_csv/small.csv', schema='userIdA INT, userIdB INT, JaccardDistance FLOAT')\n",
    "    # similarity_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/q1_all_atleast5movies_top100/q1_results.csv', schema='userIdA INT, userIdB INT, JaccardDistance FLOAT')\n",
    "    similarity_df = similarity_df.drop('JaccardDistance')\n",
    "    sim_user_df = (similarity_df.select(\"userIdA\")\n",
    "            .union(similarity_df.select(\"userIdB\"))\n",
    "            .distinct()\n",
    "            .filter(\"userIdA is not null\"))\n",
    "\n",
    "#     sim_user_df.show(50) # 50 unique userids\n",
    "#     print(sim_user_ids.count())\n",
    "    \n",
    "\n",
    "    ratings_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/ml-latest-small/ratings.csv', schema='userId INT, movieId STRING, rating FLOAT, timestamp BIGINT')\n",
    "    # ratings_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/ml-latest/ratings.csv', schema='userId INT, movieId STRING, rating FLOAT, timestamp BIGINT')\n",
    "    ratings_df = ratings_df.drop('timestamp')\n",
    "    \n",
    "    sim_filtered_ratings_df = ratings_df.join(sim_user_df, (ratings_df.userId == sim_user_df.userIdA), \"inner\").drop('userIdA').cache()\n",
    "#     sim_filtered_ratings_df.select(\"userId\").distinct().show(50) #checked all unique user's ratings are here\n",
    "#     sim_filtered_ratings_df.show(50)\n",
    "\n",
    "    common_ratings_df = sim_filtered_ratings_df.alias(\"r1\") \\\n",
    "    .join(sim_filtered_ratings_df.alias(\"r2\"),\n",
    "          (col(\"r1.movieId\") == col(\"r2.movieId\")) &\n",
    "          (col(\"r1.userId\") != col(\"r2.userId\"))) \\\n",
    "    .join(similarity_df, (col(\"r1.userId\") == col(\"userIdA\")) & (col(\"r2.userId\") == col(\"userIdB\"))) \\\n",
    "    .select(col(\"userIdA\"), col(\"userIdB\"), col(\"r1.movieId\").alias(\"movie1\"),col(\"r2.movieId\").alias(\"movie2\"),col(\"r1.rating\").alias(\"rating1\"), col(\"r2.rating\").alias(\"rating2\"))\n",
    "    \n",
    "    # Assemble ratings into vectors for correlation calculation\n",
    "    vector_assembler = VectorAssembler(inputCols=[\"rating1\", \"rating2\"], outputCol=\"features\")\n",
    "    vector_ratings_df = vector_assembler.transform(common_ratings_df)\n",
    "#     vector_ratings_df.filter(\"userIdA == 8 and userIdB == 446\").show(50) #42, verified ok\n",
    "#     vector_ratings_df = vector_ratings_df.filter(\"userIdA == 8 and userIdB == 446\")\n",
    "    vector_ratings_df = vector_ratings_df.drop(\"movie1\",\"movie2\",\"rating1\",\"rating2\")\n",
    "#     vector_ratings_df.show()\n",
    "\n",
    "\n",
    "    # Calculate correlation\n",
    "#     correlations = []\n",
    "    correlations = {}\n",
    "    for row in similarity_df.collect():\n",
    "        userA, userB = row.userIdA, row.userIdB\n",
    "        pair_data = vector_ratings_df.filter((col(\"userIdA\") == userA) & (col(\"userIdB\") == userB))\n",
    "        if not pair_data.rdd.isEmpty():\n",
    "            corr_matrix = Correlation.corr(pair_data, \"features\", \"pearson\").head()[0]\n",
    "            if corr_matrix is not None:\n",
    "                corr_value = corr_matrix[0, 1]  # Accessing the off-diagonal element for the correlation between two features\n",
    "                correlations[(userA, userB)] = corr_value\n",
    "#             correlations.append(corr)\n",
    "    \n",
    "    print(correlations)\n",
    "    average_corr = np.mean(list(correlations.values()))\n",
    "    print(f\"Average Pearson Correlation: {average_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7b48f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 10:53:59 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|userIdA|userIdB| features|\n",
      "+-------+-------+---------+\n",
      "|      8|    446|[4.0,3.0]|\n",
      "|      8|    347|[4.0,3.0]|\n",
      "|      8|     94|[4.0,4.0]|\n",
      "|      8|    446|[2.0,3.0]|\n",
      "|      8|    347|[2.0,4.0]|\n",
      "|      8|     94|[2.0,3.0]|\n",
      "|      8|    446|[4.0,4.0]|\n",
      "|      8|    347|[4.0,4.0]|\n",
      "|      8|     94|[4.0,3.0]|\n",
      "|      8|    446|[4.0,3.0]|\n",
      "|      8|    379|[4.0,2.0]|\n",
      "|      8|     94|[4.0,3.0]|\n",
      "|      8|    446|[3.0,4.0]|\n",
      "|      8|    347|[3.0,3.0]|\n",
      "|      8|     94|[3.0,5.0]|\n",
      "|      8|    446|[5.0,5.0]|\n",
      "|      8|    379|[5.0,5.0]|\n",
      "|      8|    347|[5.0,4.0]|\n",
      "|      8|     94|[5.0,4.0]|\n",
      "|      8|    446|[3.0,3.0]|\n",
      "+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "{(130, 145): -0.2966896119691211, (130, 574): 0.18051650034429462, (130, 468): 0.02099802627829038, (126, 379): 0.22243612372734356, (242, 468): 0.15144775094307247, (150, 270): 0.3138439323974279, (126, 130): -0.05551876851617387, (94, 347): -0.04621630369226584, (81, 126): 0.11490421517543613, (46, 468): 0.5166966485951882, (130, 485): -0.08684410999797731, (126, 498): 0.3682927667642311, (145, 574): -0.14725377234348785, (130, 242): 0.15374659880634958, (270, 389): 0.45382534835386923, (126, 485): 0.716511612720786, (126, 574): 0.1960161168432973, (107, 468): 0.4789837292190185, (107, 130): 0.22151900580907405, (126, 512): 0.11897061009214624, (94, 126): 0.3161836768056788, (46, 242): 0.014345346622205567, (8, 94): 0.018442777839082904, (94, 512): 0.11520238376398106, (379, 512): 0.05175202908319944, (446, 566): 0.5229806133591467, (455, 512): 0.2618495816019348, (38, 446): 0.4142768007059478, (56, 126): 0.08830876247006735, (242, 574): 0.1971556009987098, (340, 374): 0.15419848868599026, (56, 94): 0.060643164521958226, (126, 242): -0.04163139024564538, (145, 468): 0.18491929019140235, (468, 574): 0.12027195163444267, (94, 470): 0.21866277936292036, (126, 468): 0.4313607732605676, (46, 126): 0.5636214801906781, (8, 379): 0.09574749827023768, (126, 347): 0.01632340356247973, (8, 347): -0.0289372294317487, (126, 179): 0.3579141792093792, (94, 242): 0.042457837304364456, (235, 446): 0.3660693119680804, (179, 470): 0.36002251586883505, (46, 130): -0.05626988145954609, (340, 379): 0.2029640860513956, (130, 374): 0.5970018473390797, (126, 145): 0.2762894819977688, (485, 498): 0.07584950469352424, (94, 455): 0.2507644113873983, (126, 455): 0.2985656252384356, (347, 512): 0.2784037544988055, (485, 574): -0.2180257124001791, (145, 485): 0.3424747597107866, (145, 569): -0.11879774082787294, (270, 521): 0.511374417688658, (174, 446): -0.05366734884263653, (126, 176): 0.5451468250292574, (270, 451): 0.4171497586163621, (126, 374): -0.06752437482988746, (206, 270): 0.4032996434533988, (81, 379): 0.33844688100278186, (38, 566): 0.28467203970364524, (26, 126): 0.29115455694080644, (126, 507): 0.13412842031832775, (389, 529): 0.28338231844937195, (374, 569): -0.12320134507614523, (373, 446): 0.4162049066175765, (8, 446): 0.4231202707606195, (347, 470): 0.04995592382277027, (107, 242): 0.06675015657623454, (38, 455): 0.3210149295645314, (46, 56): -0.04767398939047858, (130, 498): 0.17182461898289544, (81, 94): 0.5233793327271586, (458, 470): 0.04109094055535775, (46, 94): 0.4324227303271506, (404, 455): 0.428746462856272, (126, 142): 0.023453251491009738, (229, 235): 0.41492034294049923, (126, 133): 0.3888960907350317, (94, 468): -0.0532805052618533, (94, 446): 0.05846587983437376, (126, 470): 0.3685006657080189, (446, 512): 0.3557283940422755, (240, 559): 0.195031557246057, (46, 455): 0.23847193625262306, (130, 569): 0.25492496425523054, (379, 455): 0.22861169569666523, (46, 134): 0.16474279017877488, (81, 498): 0.12183525980985443, (58, 373): 0.41634052621129025, (350, 451): -0.0743744093254076, (498, 574): 0.2908375253735555, (242, 485): -0.20534280767213314, (126, 569): 0.11099791720325179, (145, 242): 0.302613766334401, (46, 347): 0.15819838317460494, (54, 126): 0.1560976352636156}\n",
      "Average Pearson Correlation: 0.20134417118730055\n"
     ]
    }
   ],
   "source": [
    "# Only enter this block if we're in main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"correlation\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    # Call our main routine\n",
    "    main(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aeee80",
   "metadata": {},
   "source": [
    "### Q2 - order top 100 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa34d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, concat_ws, rand\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "'''all ordered top 100 correlation'''\n",
    "\n",
    "def main(spark):\n",
    "\n",
    "\n",
    "    \n",
    "    # 1. Read & Transform the data\n",
    "    similarity_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/q1_all_atleast5movies_allJD0/q1_allJD0_ordered100.csv', schema='userIdA INT, userIdB INT, JaccardDistance FLOAT')\n",
    "    similarity_df = similarity_df.drop('JaccardDistance')\n",
    "    sim_user_df = (similarity_df.select(\"userIdA\")\n",
    "            .union(similarity_df.select(\"userIdB\"))\n",
    "            .distinct()\n",
    "            .filter(\"userIdA is not null\"))\n",
    "\n",
    "#     sim_user_df.show(50) # 50 unique userids\n",
    "#     print(sim_user_ids.count())\n",
    "    \n",
    "\n",
    "    ratings_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/ml-latest/ratings.csv', schema='userId INT, movieId STRING, rating FLOAT, timestamp BIGINT')\n",
    "    ratings_df = ratings_df.drop('timestamp')\n",
    "    \n",
    "    sim_filtered_ratings_df = ratings_df.join(sim_user_df, (ratings_df.userId == sim_user_df.userIdA), \"inner\").drop('userIdA').cache()\n",
    "#     sim_filtered_ratings_df.select(\"userId\").distinct().show(50) #checked all unique user's ratings are here\n",
    "#     sim_filtered_ratings_df.show(50)\n",
    "\n",
    "    sim_common_ratings_df = sim_filtered_ratings_df.alias(\"r1\") \\\n",
    "    .join(sim_filtered_ratings_df.alias(\"r2\"),\n",
    "          (col(\"r1.movieId\") == col(\"r2.movieId\")) &\n",
    "          (col(\"r1.userId\") != col(\"r2.userId\"))) \\\n",
    "    .join(similarity_df, (col(\"r1.userId\") == col(\"userIdA\")) & (col(\"r2.userId\") == col(\"userIdB\"))) \\\n",
    "    .select(col(\"userIdA\"), col(\"userIdB\"), col(\"r1.movieId\").alias(\"movie1\"),col(\"r2.movieId\").alias(\"movie2\"),col(\"r1.rating\").alias(\"rating1\"), col(\"r2.rating\").alias(\"rating2\"))\n",
    "    \n",
    "    # Assemble ratings into vectors for correlation calculation\n",
    "    vector_assembler = VectorAssembler(inputCols=[\"rating1\", \"rating2\"], outputCol=\"features\")\n",
    "    sim_vector_ratings_df = vector_assembler.transform(sim_common_ratings_df)\n",
    "#     sim_vector_ratings_df.filter(\"userIdA == 8 and userIdB == 446\").show(50) #42, verified ok\n",
    "#     sim_vector_ratings_df = sim_vector_ratings_df.filter(\"userIdA == 8 and userIdB == 446\")\n",
    "    sim_vector_ratings_df = sim_vector_ratings_df.drop(\"movie1\",\"movie2\",\"rating1\",\"rating2\")\n",
    "#     sim_vector_ratings_df.show()\n",
    "\n",
    "\n",
    "    # Calculate correlation\n",
    "#     correlations = []\n",
    "    sim_correlations = {}\n",
    "    for row in similarity_df.collect():\n",
    "        userA, userB = row.userIdA, row.userIdB\n",
    "        pair_data = sim_vector_ratings_df.filter((col(\"userIdA\") == userA) & (col(\"userIdB\") == userB))\n",
    "        if not pair_data.rdd.isEmpty():\n",
    "            corr_matrix = Correlation.corr(pair_data, \"features\", \"pearson\").head()[0]\n",
    "            if corr_matrix is not None:\n",
    "                corr_value = corr_matrix[0, 1]  # Accessing the off-diagonal element for the correlation between two features\n",
    "                sim_correlations[(userA, userB)] = corr_value\n",
    "#             correlations.append(corr)\n",
    "\n",
    "    \n",
    "    print(sim_correlations)\n",
    "    sim_valid_corrs = [corr for corr in sim_correlations.values() if not np.isnan(corr)]\n",
    "    sim_average_corr = np.mean(sim_valid_corrs)\n",
    "    print(f\"top100 Average Pearson Correlation: {sim_average_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ee5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 07:55:28 WARN CacheManager: Asked to cache already cached data.\n",
      "24/05/14 07:55:42 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:57 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:57 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:57 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:58 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:58 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:59 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:55:59 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:56:00 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:56:00 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:56:01 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:56:01 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/14 07:56:02 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(32, 35883): 0.14976057446828847, (32, 55181): 0.5126322784957922, (32, 71690): 0.04670993664969137, (32, 84047): -0.3855907133328033, (32, 130852): 0.1292785010551571, (32, 165473): 0.2792592697663689, (32, 290077): 0.2140935739950792, (47, 8182): 0.30523384783367985, (47, 19126): -0.19462473604038072, (47, 30403): -0.10910894511799621, (47, 38669): -1.1944098013747937e-17, (47, 47033): 7.166458808248763e-17, (47, 77618): 0.8451542547285166, (47, 81753): -0.5436067275445352, (47, 82924): 0.16529490122682158, (47, 164361): -0.256917497769354, (47, 199185): 0.38729833462074165, (47, 215796): 0.3651483716701107, (47, 221493): -0.37096452512212347, (47, 223838): 0.7905694150420947, (47, 237656): 0.46625240412015695, (47, 253373): -3.670284103903167e-17, (47, 256684): 0.4637388957601683, (47, 260423): -0.8460990892938031, (47, 305650): -3.288196813495083e-17, (47, 330557): -0.5288119308086269, (158, 107199): 0.08722404533150648, (158, 129659): -0.015653689050413578, (158, 148103): 0.7937282922592666, (158, 156205): nan, (158, 183097): 0.26977079219858124, (158, 281545): -0.6799001036500849, (158, 291021): -0.061429511683395145, (287, 894): -0.3952847075210474, (287, 13845): 0.42257712736425823, (287, 33470): -0.6201736729460422, (287, 53713): -0.42257712736425823, (287, 59039): 0.08625819491779425, (287, 60507): 0.42257712736425823, (287, 75023): 0.7150969419341942, (287, 88779): -0.9325048082403138, (287, 99018): 0.3952847075210474, (287, 126187): 0.8451542547285165, (287, 144934): 0.7395099728874519, (287, 149298): -0.21128856368212914, (287, 166734): 0.6454972243679028, (287, 171290): 0.8451542547285165, (287, 175581): 0.0, (287, 198983): 0.2446230273950408, (287, 203878): 0.5114957546028551, (287, 217486): -0.061360086709227, (287, 221644): 0.3952847075210474, (287, 234940): 0.7905694150420948, (287, 238221): 0.3670126371966896, (287, 278446): -0.17000510022951149, (287, 298138): -0.35497614640441544, (287, 304393): 0.6454972243679027, (287, 311301): 0.6454972243679028, (287, 314723): 0.7905694150420948, (331, 10498): nan, (331, 19381): nan, (331, 37999): nan, (331, 52118): nan, (331, 89705): nan, (331, 95410): nan, (331, 104798): nan, (331, 161766): nan, (331, 209062): nan, (331, 212264): nan, (331, 265054): nan, (331, 313896): nan, (428, 4172): 0.7441320902087377, (428, 9481): 0.32302914123489934, (428, 11821): -0.07881104062391008, (428, 13225): 0.49275690208103134, (428, 13343): 0.36115755925730764, (428, 15232): 0.4355724051843767, (428, 15318): 0.09652341781316813, (428, 19955): -0.7223151185146153, (428, 21045): 0.7268155677785234, (428, 21120): -0.613755368346309, (428, 21743): 0.7441320902087377, (428, 22381): 0.32302914123489934, (428, 26879): -0.3158231237960525, (428, 29009): 0.6035874759133595, (428, 30002): 0.8075728530872482, (428, 31036): 0.3672793098117979, (428, 31199): -0.8075728530872482, (428, 32311): -0.8687107603185125, (428, 33107): 0.7661308776828739, (428, 35581): 0.7332433506283716, (428, 35729): 0.613155020597473, (428, 35852): -0.12038585308576923, (428, 36029): -0.31648574844015015, (428, 36694): -0.48527814760823007, (428, 39229): -0.6304883249912804, (428, 39612): -0.14350946197048198, (428, 41037): 0.38597116016875255, (428, 44035): -0.613155020597473, (428, 44861): -0.32302914123489934}\n",
      "top100 Average Pearson Correlation: 0.12761721400388223\n"
     ]
    }
   ],
   "source": [
    "# Only enter this block if we're in main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"correlation all similar\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    # Call our main routine\n",
    "    main(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184acd40",
   "metadata": {},
   "source": [
    "### Q2 - not ordered top 100 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a18c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, concat_ws, rand\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def main(spark):\n",
    "\n",
    "'''all top 100 correlation'''\n",
    "    \n",
    "    # 1. Read & Transform the data\n",
    "    similarity_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/q1_all_atleast5movies_top100/q1_results.csv', schema='userIdA INT, userIdB INT, JaccardDistance FLOAT')\n",
    "    similarity_df = similarity_df.drop('JaccardDistance')\n",
    "    sim_user_df = (similarity_df.select(\"userIdA\")\n",
    "            .union(similarity_df.select(\"userIdB\"))\n",
    "            .distinct()\n",
    "            .filter(\"userIdA is not null\"))\n",
    "\n",
    "#     sim_user_df.show(50) # 50 unique userids\n",
    "#     print(sim_user_ids.count())\n",
    "    \n",
    "\n",
    "    ratings_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/ml-latest/ratings.csv', schema='userId INT, movieId STRING, rating FLOAT, timestamp BIGINT')\n",
    "    ratings_df = ratings_df.drop('timestamp')\n",
    "    \n",
    "    sim_filtered_ratings_df = ratings_df.join(sim_user_df, (ratings_df.userId == sim_user_df.userIdA), \"inner\").drop('userIdA').cache()\n",
    "#     sim_filtered_ratings_df.select(\"userId\").distinct().show(50) #checked all unique user's ratings are here\n",
    "#     sim_filtered_ratings_df.show(50)\n",
    "\n",
    "    sim_common_ratings_df = sim_filtered_ratings_df.alias(\"r1\") \\\n",
    "    .join(sim_filtered_ratings_df.alias(\"r2\"),\n",
    "          (col(\"r1.movieId\") == col(\"r2.movieId\")) &\n",
    "          (col(\"r1.userId\") != col(\"r2.userId\"))) \\\n",
    "    .join(similarity_df, (col(\"r1.userId\") == col(\"userIdA\")) & (col(\"r2.userId\") == col(\"userIdB\"))) \\\n",
    "    .select(col(\"userIdA\"), col(\"userIdB\"), col(\"r1.movieId\").alias(\"movie1\"),col(\"r2.movieId\").alias(\"movie2\"),col(\"r1.rating\").alias(\"rating1\"), col(\"r2.rating\").alias(\"rating2\"))\n",
    "    \n",
    "    # Assemble ratings into vectors for correlation calculation\n",
    "    vector_assembler = VectorAssembler(inputCols=[\"rating1\", \"rating2\"], outputCol=\"features\")\n",
    "    sim_vector_ratings_df = vector_assembler.transform(sim_common_ratings_df)\n",
    "#     sim_vector_ratings_df.filter(\"userIdA == 8 and userIdB == 446\").show(50) #42, verified ok\n",
    "#     sim_vector_ratings_df = sim_vector_ratings_df.filter(\"userIdA == 8 and userIdB == 446\")\n",
    "    sim_vector_ratings_df = sim_vector_ratings_df.drop(\"movie1\",\"movie2\",\"rating1\",\"rating2\")\n",
    "#     sim_vector_ratings_df.show()\n",
    "\n",
    "\n",
    "    # Calculate correlation\n",
    "#     correlations = []\n",
    "    sim_correlations = {}\n",
    "    for row in similarity_df.collect():\n",
    "        userA, userB = row.userIdA, row.userIdB\n",
    "        pair_data = sim_vector_ratings_df.filter((col(\"userIdA\") == userA) & (col(\"userIdB\") == userB))\n",
    "        if not pair_data.rdd.isEmpty():\n",
    "            corr_matrix = Correlation.corr(pair_data, \"features\", \"pearson\").head()[0]\n",
    "            if corr_matrix is not None:\n",
    "                corr_value = corr_matrix[0, 1]  # Accessing the off-diagonal element for the correlation between two features\n",
    "                sim_correlations[(userA, userB)] = corr_value\n",
    "#             correlations.append(corr)\n",
    "\n",
    "    \n",
    "    print(correlations)\n",
    "    sim_valid_corrs = [corr for corr in sim_correlations.values() if not np.isnan(corr)]\n",
    "    sim_average_corr = np.mean(sim_valid_corrs)\n",
    "    print(f\"top100 Average Pearson Correlation: {sim_average_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c2893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/05/11 12:44:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 12:44:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/11 12:44:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/05/11 12:45:02 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/11 12:45:11 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/11 12:45:33 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/11 12:45:35 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(126365, 212700): -0.2041241452319315, (19955, 221069): -0.7954951288348662, (157346, 243115): 0.30541548060657264, (116983, 193321): -0.060999428133041905, (11821, 290387): -0.2672612419124244, (6282, 42342): 0.27116307227332015, (11821, 256886): -0.17496355305594133, (126365, 164050): -0.26404445070327603, (19955, 129257): -0.5570860145311556, (123839, 254507): 0.20806259464411975, (19955, 29009): -0.5570860145311556, (226098, 278393): 0.47540983606557385, (19955, 269153): 0.0, (11308, 302349): -0.06843859108291546, (100351, 174117): -0.008261359951610172, (60507, 166734): 0.7637626158259734, (45553, 297891): 0.6507913734559682, (125979, 241804): -0.4391550328268399, (186881, 266885): 0.2581988897471611, (88779, 314723): -0.884651736929383, (126365, 314990): -0.4227971623322353, (76751, 117965): -0.42379344841321, (90557, 161092): -0.2536857024815635, (177400, 267170): -0.3306500356330541, (126365, 281537): 0.19094065395649326, (188488, 289956): 0.316227766016838, (165829, 307269): -3.2667751078491157e-17, (58395, 151454): -0.10084389681792218, (129620, 224220): 0.14744195615489716, (63888, 258064): 0.0, (30709, 277281): -0.39149681322891033, (44297, 308060): 0.2936101097573518, (7796, 214797): nan, (167108, 195712): -0.22473328748774735, (143490, 328181): -0.408248290463863, (35071, 64177): -0.3721740365924434, (73802, 186012): 0.7484298895080298, (69150, 281780): -0.6666666666666666, (59039, 60507): 0.7581753965757454, (161522, 264699): -0.16620562382863338, (894, 59039): -0.49099025303098276, (30821, 44297): 0.46945692571039066, (88779, 304393): -0.8427009716003844, (263780, 266311): nan, (207718, 273406): 0.12771017136282017, (156707, 239356): 0.1492555785314984, (284794, 300978): 0.17245936904743847, (96382, 270339): -0.1432082875315512, (76751, 204665): 0.4494665749754947, (139898, 288245): 0.7808688094430303, (76751, 320941): -0.38924947208076155, (156027, 266989): 0.17770466332772772, (141384, 228812): 0.2195175913660583, (152985, 276454): 0.6154463604757145, (164176, 223914): 0.3114205494515248, (36694, 46567): -0.23252277336018531, (195610, 318044): 0.4890096469218258, (187822, 269153): -0.727606875108999, (98616, 167982): 0.653358607776565, (46567, 199530): 0.013296743760228538, (51213, 203266): 0.6745134750686498, (33107, 51191): -0.1942571724714529, (49517, 249856): 0.48894891169872357, (76221, 153651): 0.14002800840280097, (66369, 138608): -0.6614378277661476, (161179, 164943): 0.28306925853614895, (116010, 242055): -0.992230545278279, (56874, 187822): 0.6301260378126043, (56874, 173828): 0.7216878364870322, (51191, 187822): 0.16657415116319244, (35219, 290367): 0.32076651393589234, (46593, 148056): 0.7985836518841365, (35219, 281780): -0.1454785934906616, (44861, 155429): 0.0, (167108, 224093): nan, (165294, 229294): -0.31269686276826975, (46392, 229090): nan, (170628, 283348): -0.26352313834736496, (128399, 308060): 0.0, (35071, 76174): -0.3993529248931195, (208072, 288245): 0.0894427190999916, (42024, 268661): -0.2706040365966953, (45868, 173828): -0.13736056394868906, (11308, 275203): 0.16529490122682158, (4172, 35852): -0.13736056394868906, (88779, 198983): 0.1064523363176829, (4172, 188166): -0.46137365028977817, (195610, 220926): -0.4376068506549086, (4172, 275327): 0.6006361710360177, (188488, 199530): 0.37796447300922725, (156027, 261764): -0.26967994498529685, (152350, 214369): 0.0, (22381, 30002): 0.65, (214797, 281780): 0.21821789023599247, (22381, 147287): -0.3730019232961256, (278393, 289528): 0.42796533092892686, (22381, 260366): -0.7745966692414833, (30821, 281905): 0.2679783730127122, (152985, 261764): -0.06773825702985774, (28772, 95888): 0.32025630761017426}\n",
      "Average Pearson Correlation: 0.007267372445985579\n"
     ]
    }
   ],
   "source": [
    "# Only enter this block if we're in main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"correlation all similar\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    # Call our main routine\n",
    "    main(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcecd1",
   "metadata": {},
   "source": [
    "### Q2 random 100 correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7d7624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, concat_ws, rand, collect_list, size\n",
    "\n",
    "'''Random 100 correlation'''\n",
    "\n",
    "def main(spark):\n",
    "\n",
    "    \n",
    "    # 1. Read & Transform the data\n",
    "    ratings_df = spark.read.csv(f'/home/hl5679/capstone-project-cap-19/ml-latest-small/ratings.csv', schema='userId INT, movieId STRING, rating FLOAT, timestamp BIGINT')\n",
    "    ratings_df = ratings_df.drop('timestamp')\n",
    "    \n",
    "    # Create random user pairs\n",
    "    ratings_df_filtered = ratings_df.groupBy(\"userId\").agg(collect_list(\"movieId\").alias(\"movieIds\")).filter(size(\"movieIds\") >= 5)\n",
    "#     ratings_df_filtered.show()\n",
    "    user_ids = ratings_df_filtered.select(\"userId\").distinct()\n",
    "    user_pairs = user_ids.alias(\"a\").crossJoin(user_ids.alias(\"b\")) \\\n",
    "        .filter(\"a.userId < b.userId\") \\\n",
    "        .select(col(\"a.userId\").alias(\"userIdA\"), col(\"b.userId\").alias(\"userIdB\"))\n",
    "    \n",
    "    random_pairs = user_pairs.orderBy(rand()).limit(100)\n",
    "    random_pairs.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    ran_user_df = (random_pairs.select(\"userIdA\")\n",
    "            .union(random_pairs.select(\"userIdB\"))\n",
    "            .distinct()\n",
    "            .filter(\"userIdA is not null\"))\n",
    "    \n",
    "    \n",
    "    ran_filtered_ratings_df = ratings_df.join(ran_user_df, (ratings_df.userId == ran_user_df.userIdA), \"inner\").drop('userIdA').cache()\n",
    "#     ran_filtered_ratings_df.select(\"userId\").distinct().show(50) #checked all unique user's ratings are here\n",
    "#     ran_filtered_ratings_df.show(50)\n",
    "\n",
    "    ran_common_ratings_df = ran_filtered_ratings_df.alias(\"r1\") \\\n",
    "    .join(ran_filtered_ratings_df.alias(\"r2\"),\n",
    "          (col(\"r1.movieId\") == col(\"r2.movieId\")) &\n",
    "          (col(\"r1.userId\") != col(\"r2.userId\"))) \\\n",
    "    .join(random_pairs, (col(\"r1.userId\") == col(\"userIdA\")) & (col(\"r2.userId\") == col(\"userIdB\"))) \\\n",
    "    .select(col(\"userIdA\"), col(\"userIdB\"), col(\"r1.movieId\").alias(\"movie1\"),col(\"r2.movieId\").alias(\"movie2\"),col(\"r1.rating\").alias(\"rating1\"), col(\"r2.rating\").alias(\"rating2\"))\n",
    "    \n",
    "    # Assemble ratings into vectors for correlation calculation\n",
    "    vector_assembler = VectorAssembler(inputCols=[\"rating1\", \"rating2\"], outputCol=\"features\")\n",
    "    ran_vector_ratings_df = vector_assembler.transform(ran_common_ratings_df)\n",
    "    ran_vector_ratings_df = ran_vector_ratings_df.drop(\"movie1\",\"movie2\",\"rating1\",\"rating2\")\n",
    "    ran_vector_ratings_df.show()\n",
    "\n",
    "\n",
    "    # Calculate correlation\n",
    "    ran_correlations = {}\n",
    "    for row in random_pairs.collect():\n",
    "        userA, userB = row.userIdA, row.userIdB\n",
    "        ran_pair_data = ran_vector_ratings_df.filter((col(\"userIdA\") == userA) & (col(\"userIdB\") == userB))\n",
    "        if ran_pair_data.count() >= 2:\n",
    "            if not ran_pair_data.rdd.isEmpty():\n",
    "                corr_matrix = Correlation.corr(ran_pair_data, \"features\", \"pearson\").head()[0] #Correlation.corr function requires at least two rows to compute the covariance matrix,\n",
    "                if corr_matrix is not None:\n",
    "                    corr_value = corr_matrix[0, 1]  # Accessing the off-diagonal element for the correlation between two features\n",
    "                    ran_correlations[(userA, userB)] = corr_value\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "#         print(\"Number of rows in the DataFrame:\", ran_pair_data.count())\n",
    "\n",
    "\n",
    "    \n",
    "    print(ran_correlations)\n",
    "    ran_valid_corrs = [corr for corr in ran_correlations.values() if not np.isnan(corr)]\n",
    "    ran_average_corr = np.mean(ran_valid_corrs)\n",
    "    print(f\"ran100 Average Pearson Correlation: {ran_average_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f76d0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|userIdA|userIdB|\n",
      "+-------+-------+\n",
      "|      6|    130|\n",
      "|     53|    480|\n",
      "|     86|    321|\n",
      "|    328|    555|\n",
      "|    118|    251|\n",
      "|     22|    269|\n",
      "|    221|    452|\n",
      "|     51|     89|\n",
      "|     46|    308|\n",
      "|    124|    549|\n",
      "|    186|    269|\n",
      "|    172|    379|\n",
      "|    157|    546|\n",
      "|     90|    116|\n",
      "|      5|    507|\n",
      "|      8|    419|\n",
      "|     46|    388|\n",
      "|    519|    574|\n",
      "|    112|    363|\n",
      "|    156|    417|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|userIdA|userIdB| features|\n",
      "+-------+-------+---------+\n",
      "|    328|    555|[3.5,4.0]|\n",
      "|      6|    130|[2.0,4.0]|\n",
      "|      8|    419|[4.0,5.0]|\n",
      "|     17|    233|[5.0,4.0]|\n",
      "|     42|    566|[5.0,4.0]|\n",
      "|     58|    477|[5.0,4.5]|\n",
      "|     67|    304|[4.5,5.0]|\n",
      "|    118|    251|[4.0,5.0]|\n",
      "|    140|    166|[4.0,4.5]|\n",
      "|    156|    417|[5.0,5.0]|\n",
      "|    173|    588|[1.0,5.0]|\n",
      "|    192|    385|[3.0,4.0]|\n",
      "|    275|    290|[5.0,5.0]|\n",
      "|    290|    294|[5.0,4.0]|\n",
      "|    328|    555|[5.0,4.0]|\n",
      "|    330|    418|[4.0,4.5]|\n",
      "|    334|    353|[4.0,5.0]|\n",
      "|    359|    609|[4.0,4.0]|\n",
      "|    404|    416|[3.0,4.0]|\n",
      "|    411|    606|[5.0,5.0]|\n",
      "+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 13:49:39 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/11 13:49:50 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/11 13:53:45 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "24/05/11 13:53:55 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(6, 130): -0.2533406866391176, (53, 480): nan, (86, 321): -0.4264014327112209, (328, 555): -0.04175932386355734, (118, 251): nan, (221, 452): 0.030987092168430933, (51, 89): -0.1804311640889395, (46, 308): 0.15196658067558388, (124, 549): 0.1889822365046136, (186, 269): 0.0, (172, 379): -0.9999999999999998, (90, 116): 0.9999999999999998, (5, 507): 0.26038690306103024, (8, 419): 0.2842676218074807, (112, 363): -0.1590958925924491, (156, 417): 0.11411646470852956, (274, 320): -0.2770979616076669, (359, 609): 0.46291004988627577, (95, 558): 0.777713771047819, (156, 361): -0.13873210524018872, (196, 570): -0.08799637965199711, (411, 606): 0.03383518421552754, (32, 112): 0.4631820399782618, (425, 443): 0.06465130348292425, (314, 435): 0.9759000729485333, (44, 195): -0.5477225575051662, (267, 301): -0.06565321642986127, (392, 534): 0.7196763181246417, (77, 351): 0.36261333437825766, (17, 233): 0.5749889084999453, (79, 372): 0.08189371934394457, (39, 350): 0.1856953381770519, (190, 426): 0.34838003673033907, (332, 452): 0.2560136881987638, (42, 180): 0.23306863292670024, (235, 316): -1.0, (48, 101): 1.0, (290, 294): 0.3888115555611101, (140, 166): -0.06321585163998387, (515, 606): -0.2185433458832612, (489, 606): 0.38847277962024696, (195, 452): 0.010352565152872015, (125, 258): 0.47060485181697004, (139, 505): -1.0000000000000002, (46, 59): 0.04583492485141061, (258, 344): 0.7022602074068445, (347, 423): 0.509524665365068, (80, 510): 0.2144761058952721, (39, 554): 0.41204282171516443, (42, 566): 0.08993061429918968, (192, 385): 0.456530545406207, (404, 416): -0.3535533905932738, (58, 477): 0.3135825418686618, (334, 353): -0.2077782620184336, (136, 344): 0.5000000000000001, (366, 561): 0.15452877499092174, (448, 561): 0.37481407258182337, (173, 588): -0.2257234745358171, (275, 290): -0.04262192346202062, (171, 461): 0.9890707100936807, (437, 511): 1.0, (268, 474): 0.3833698693144745, (330, 418): 0.6895956514188492, (39, 547): nan, (311, 317): -0.28389613404443026, (67, 304): -0.1074003720881377, (150, 267): nan, (552, 556): -0.8660254037844387, (272, 318): -0.12909944487358058, (184, 490): 0.8513518985456004, (258, 268): 1.0, (31, 341): 0.6666666666666665}\n",
      "ran100 Average Pearson Correlation: 0.1692200411203257\n"
     ]
    }
   ],
   "source": [
    "# Only enter this block if we're in main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"correlation all similar\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    # Call our main routine\n",
    "    main(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988e143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
